{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5U1bPetVYK4G",
    "colab_type": "text"
   },
   "source": [
    "# Start Tensorflow with starttf\n",
    "\n",
    "First we need to install tensorflow and check if it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YKRSAMwmWWzn",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "#pip install tensorflow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "D7gE9-ECjngu",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "outputId": "a853c14b-46af-417b-aeee-37f05bf45c77",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.526143630318E12,
     "user_tz": -120.0,
     "elapsed": 750.0,
     "user": {
      "displayName": "Michael PenguinMenace",
      "photoUrl": "//lh3.googleusercontent.com/-wv0FkQOEnjg/AAAAAAAAAAI/AAAAAAAAAHM/byTo6iipigo/s50-c-k-no/photo.jpg",
      "userId": "112772895502545919169"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLjN3csHXFf2",
    "colab_type": "text"
   },
   "source": [
    "Next let's install starttf and opendatalake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "Y8HotiWuYv1p",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "#pip uninstall -y starttf\n",
    "#pip uninstall -y opendatalake\n",
    "\n",
    "#pip install https://github.com/penguinmenac3/starttf/archive/master.zip\n",
    "#pip install https://github.com/penguinmenac3/opendatalake/archive/master.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "as-_GHVsY-LQ",
    "colab_type": "text"
   },
   "source": [
    "# Starting out\n",
    "\n",
    "## Loading a dataset\n",
    "\n",
    "Let's start by loading a dataset.\n",
    "The simplest dataset for beginners is mnist.\n",
    "The good thing about mnist is, that you do not need any complex downloading code.\n",
    "Simply load it via the opendatalake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "LNlTt3s5b8g0",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "from opendatalake.classification.mnist import mnist\n",
    "\n",
    "base_dir = \"data/mnist\"\n",
    "\n",
    "# Get a generator and its parameters\n",
    "train_gen, train_gen_params = mnist(base_dir=base_dir, phase=\"train\")\n",
    "validation_gen, validation_gen_params = mnist(base_dir=base_dir, phase=\"validation\")\n",
    "\n",
    "# Create a generator to see some images\n",
    "data = train_gen(train_gen_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-OR1AOQdDbF",
    "colab_type": "text"
   },
   "source": [
    "Now you have downloaded the dataset and a generator which will output you labels and features.\n",
    "Let's inspect some features and labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "yjgnNJZWdML9",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     },
     "base_uri": "https://localhost:8080/",
     "height": 51.0
    },
    "outputId": "a59aa157-56ba-4a8a-ea07-c32d5e5334f4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.526143650537E12,
     "user_tz": -120.0,
     "elapsed": 511.0,
     "user": {
      "displayName": "Michael PenguinMenace",
      "photoUrl": "//lh3.googleusercontent.com/-wv0FkQOEnjg/AAAAAAAAAAI/AAAAAAAAAHM/byTo6iipigo/s50-c-k-no/photo.jpg",
      "userId": "112772895502545919169"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image'])\n",
      "dict_keys(['probs'])\n"
     ]
    }
   ],
   "source": [
    "features, labels = next(data)\n",
    "print(features.keys())\n",
    "print(labels.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PbyB-19rdazt",
    "colab_type": "text"
   },
   "source": [
    "The image is a numpy array. Let's plot it using matplotlib. The label is one hot encoded probabilities. Using np.argmax you can receive the label of the image.\n",
    "\n",
    "In the case of mnist the image is a 1d-array with 786 values. However, it actually represents a (28,28) image. So first you have to reshape it using numpy.\n",
    "\n",
    "The Label is one hot encoded, this means you can use `np.argmax` to retrieve the index at which the one is (aka the label in human readable form).\n",
    "\n",
    "Finally plot the image using matplotlibs imshow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "eVREumamdY8d",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     },
     "base_uri": "https://localhost:8080/",
     "height": 362.0
    },
    "outputId": "57aa54b7-a347-4f85-a710-e94438d95dca",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.526143652914E12,
     "user_tz": -120.0,
     "elapsed": 540.0,
     "user": {
      "displayName": "Michael PenguinMenace",
      "photoUrl": "//lh3.googleusercontent.com/-wv0FkQOEnjg/AAAAAAAAAAI/AAAAAAAAAHM/byTo6iipigo/s50-c-k-no/photo.jpg",
      "userId": "112772895502545919169"
     }
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFZCAYAAAARqQ0OAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcf955b3470>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reshape image from (786,) to (28,28)\n",
    "img = np.reshape(features['image'], (28,28))\n",
    "number = np.argmax(labels['probs'])\n",
    "\n",
    "# Plot img with number as title\n",
    "plt.title(\"Number: %d\" % number)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FkCrnqEgz3x",
    "colab_type": "text"
   },
   "source": [
    "## Hyperparameters object\n",
    "\n",
    "We need a hyperparams object to store all hyperparamters we setup for our training. Let's create one where we can add all variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "aTkJkkc3g90g",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "from starttf.utils.dict2obj import Dict2Obj\n",
    "\n",
    "hyper_params = Dict2Obj({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImAClLTFfFc1",
    "colab_type": "text"
   },
   "source": [
    "## Preparing a training\n",
    "\n",
    "Before writing a model, loss and everything fancy, you need to prepare your data for training.\n",
    "\n",
    "In the case of mnist no cleaning or augmentation is required, so we can simply write the data into a tfrecord file.\n",
    "\n",
    "However, to illustrate how data augmentation could work, we will set the data augmentation steps to 1 manually.\n",
    "This involves adding a problem parameter to our hyper parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "y5CfwwFgf_he",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     },
     "base_uri": "https://localhost:8080/",
     "height": 986.0
    },
    "outputId": "fe03fc4d-6128-4a02-e8cc-3c211af24aed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples written by records/train_0.tfrecords:thread_0: 1000.\n",
      "Samples written by records/train_0.tfrecords:thread_0: 4000.\n",
      "Samples written by records/train_0.tfrecords:thread_0: 2000.\n",
      "Samples written by records/train_0.tfrecords:thread_0: 3000.\n",
      "Samples written by records/train_0.tfrecords:thread_0: 5000.\n",
      "Samples written by records/train_0.tfrecords:thread_0: 6000.\n",
      "Samples written by records/train_0.tfrecords:thread_0: 7000.\n",
      "Samples written by records/train_0.tfrecords:thread_0: 8000.\n",
      "Samples written by records/train_0.tfrecords:thread_0: 9000.\n",
      "Samples written by records/train_0.tfrecords:thread_0: 10000.\n",
      "Samples written by records/train_0.tfrecords:thread_0: 11000.\n",
      "Samples written by records/train_0.tfrecords:thread_0: 12000.\n",
      "Samples written by records/train_0.tfrecords:thread_0: 13000.\n",
      "Samples written by records/train_0.tfrecords:thread_0: 13750.\n",
      "Samples written by records/train_1.tfrecords:thread_1: 1000.\n",
      "Samples written by records/train_1.tfrecords:thread_1: 2000.\n",
      "Samples written by records/train_1.tfrecords:thread_1: 3000.\n",
      "Samples written by records/train_1.tfrecords:thread_1: 4000.\n",
      "Samples written by records/train_1.tfrecords:thread_1: 5000.\n",
      "Samples written by records/train_1.tfrecords:thread_1: 6000.\n",
      "Samples written by records/train_2.tfrecords:thread_2: 1000.\n",
      "Samples written by records/train_2.tfrecords:thread_2: 2000.\n",
      "Samples written by records/train_1.tfrecords:thread_1: 7000.\n",
      "Samples written by records/train_3.tfrecords:thread_3: 1000.\n",
      "Samples written by records/train_2.tfrecords:thread_2: 3000.\n",
      "Samples written by records/train_1.tfrecords:thread_1: 8000.\n",
      "Samples written by records/train_2.tfrecords:thread_2: 4000.\n",
      "Samples written by records/train_3.tfrecords:thread_3: 2000.\n",
      "Samples written by records/train_2.tfrecords:thread_2: 5000.\n",
      "Samples written by records/train_1.tfrecords:thread_1: 9000.\n",
      "Samples written by records/train_3.tfrecords:thread_3: 3000.\n",
      "Samples written by records/train_2.tfrecords:thread_2: 6000.\n",
      "Samples written by records/train_2.tfrecords:thread_2: 7000.\n",
      "Samples written by records/train_3.tfrecords:thread_3: 4000.\n",
      "Samples written by records/train_1.tfrecords:thread_1: 10000.\n",
      "Samples written by records/train_3.tfrecords:thread_3: 5000.\n",
      "Samples written by records/train_2.tfrecords:thread_2: 8000.\n",
      "Samples written by records/train_3.tfrecords:thread_3: 6000.\n",
      "Samples written by records/train_1.tfrecords:thread_1: 11000.\n",
      "Samples written by records/train_2.tfrecords:thread_2: 9000.\n",
      "Samples written by records/train_3.tfrecords:thread_3: 7000.\n",
      "Samples written by records/train_1.tfrecords:thread_1: 12000.\n",
      "Samples written by records/train_2.tfrecords:thread_2: 10000.\n",
      "Samples written by records/train_3.tfrecords:thread_3: 8000.\n",
      "Samples written by records/train_2.tfrecords:thread_2: 11000.\n",
      "Samples written by records/train_1.tfrecords:thread_1: 13000.\n",
      "Samples written by records/train_3.tfrecords:thread_3: 9000.\n",
      "Samples written by records/train_1.tfrecords:thread_1: 13750.\n",
      "Samples written by records/train_2.tfrecords:thread_2: 12000.\n",
      "Samples written by records/train_3.tfrecords:thread_3: 10000.\n",
      "Samples written by records/train_2.tfrecords:thread_2: 13000.\n",
      "Samples written by records/train_3.tfrecords:thread_3: 11000.\n",
      "Samples written by records/train_2.tfrecords:thread_2: 13750.\n",
      "Samples written by records/train_3.tfrecords:thread_3: 12000.\n",
      "Samples written by records/train_3.tfrecords:thread_3: 13000.\n",
      "Samples written by records/train_3.tfrecords:thread_3: 13750.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples written by records/validation_0.tfrecords:thread_0: 1000.\n"
     ]
    }
   ],
   "source": [
    "from starttf.tfrecords.autorecords import write_data\n",
    "\n",
    "write_data(hyper_params, \"records/train\", train_gen, train_gen_params, 4)\n",
    "write_data(hyper_params, \"records/validation\", validation_gen, validation_gen_params, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbYrSStLwzpU",
    "colab_type": "text"
   },
   "source": [
    "## Training a model (the easy way)\n",
    "\n",
    "Now that the data is written into a format that we can efficiently read for training, let's have a look at an easy way to train a model.\n",
    "\n",
    "First let's create a dict where to gather all hyperparameters for training. Usually you would put that in an extra .json file which can be loaded easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "8cw2zI7uyXet",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "hyper_params_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOmUzd22ybaZ",
    "colab_type": "text"
   },
   "source": [
    "For simplicity we will use a predefined model (we will later see how to write a create model function by ourselves).\n",
    "\n",
    "This model has a hyperparameter for `dropout_rate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "lXeNw6YAxNDw",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "from starttf.models.mnist import create_model as mnist_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wbIGqcsxXLd",
    "colab_type": "text"
   },
   "source": [
    "Next we need to define a loss.\n",
    "\n",
    "The loss glues together our labels and the model.\n",
    "\n",
    "In the case of mnist it is a simple cross entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "h1hHVuwoxVkT",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "def create_loss(model, labels, mode, hyper_params):\n",
    "    \"\"\"\n",
    "    Create a cross entropy loss with the loss as the only metric.\n",
    "\n",
    "    :param model: A dictionary containing all output tensors of your model.\n",
    "    :param labels: A dictionary containing all label tensors.\n",
    "    :param mode: tf.estimators.ModeKeys defining if you are in eval or training mode.\n",
    "    :param hyper_params: A hyper parameters object.\n",
    "    :return: All the losses (tensor dict, \"loss\" is the loss that is used for minimization)\n",
    "            and all the metrics(tensor dict) that should be logged for debugging.\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    losses = {}\n",
    "\n",
    "    # Add loss\n",
    "    labels = tf.reshape(labels[\"probs\"], [-1, hyper_params.problem.number_of_categories])\n",
    "    ce = tf.nn.softmax_cross_entropy_with_logits_v2(logits=model[\"logits\"], labels=labels)\n",
    "    loss_op = tf.reduce_mean(ce)\n",
    "\n",
    "    # Add losses to dict. \"loss\" is the primary loss that is optimized.\n",
    "    losses[\"loss\"] = loss_op\n",
    "    metrics['accuracy'] = tf.metrics.accuracy(labels=labels,\n",
    "                                              predictions=model[\"probs\"],\n",
    "                                              name='acc_op')\n",
    "\n",
    "    return losses, metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbGeBGXxxVYd",
    "colab_type": "text"
   },
   "source": [
    "Now we need to define all our hyperparameters and launch the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "dqggqmHNxv3D",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     },
     "base_uri": "https://localhost:8080/",
     "height": 2495.0
    },
    "outputId": "0bf92937-b306-423f-a512-89c11ab664ff",
    "executionInfo": {
     "status": "error",
     "timestamp": 1.526143493896E12,
     "user_tz": -120.0,
     "elapsed": 123845.0,
     "user": {
      "displayName": "Michael PenguinMenace",
      "photoUrl": "//lh3.googleusercontent.com/-wv0FkQOEnjg/AAAAAAAAAAI/AAAAAAAAAHM/byTo6iipigo/s50-c-k-no/photo.jpg",
      "userId": "112772895502545919169"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'checkpoints/2018-05-12_16.42.51', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 200, '_save_checkpoints_secs': None, '_session_config': None, '_keep_checkpoint_max': 200, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff6778ca2b0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 120 secs (eval_spec.throttle_secs) or training is finished.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into checkpoints/2018-05-12_16.42.51/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.2878337, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 39 into checkpoints/2018-05-12_16.42.51/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.3775808.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-05-12-16:44:53\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/2018-05-12_16.42.51/model.ckpt-39\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "eval: Step 39, Loss 0.34733086824417114\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "ignored",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-8c6d6869a600>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstarttf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscientific_estimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0measy_train_and_evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0measy_train_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyper_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmnist_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minline_plotting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/starttf/estimators/scientific_estimator.py\u001b[0m in \u001b[0;36measy_train_and_evaluate\u001b[0;34m(hyper_params, create_model, create_loss, init_model, inline_plotting)\u001b[0m\n\u001b[1;32m    151\u001b[0m                                       throttle_secs=120)\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(estimator, train_spec, eval_spec)\u001b[0m\n\u001b[1;32m    437\u001b[0m         '(with task id 0).  Given task id {}'.format(config.task_id))\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m   \u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    516\u001b[0m         config.task_type != run_config_lib.TaskType.EVALUATOR):\n\u001b[1;32m    517\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running training and evaluation locally (non-distributed).'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m       \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py\u001b[0m in \u001b[0;36mrun_local\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    655\u001b[0m       \u001b[0;31m# condition is satisfied (both checks use the same global_step value,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m       \u001b[0;31m# i.e., no race condition)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m       \u001b[0meval_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_and_export\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0meval_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_EvalStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEVALUATED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/training.py\u001b[0m in \u001b[0;36mevaluate_and_export\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    845\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eval_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m           \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlatest_ckpt_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m           hooks=self._eval_spec.hooks)\n\u001b[0m\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m       \u001b[0;31m# _EvalResult validates the metrics.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, input_fn, steps, hooks, checkpoint_path, name)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_convert_eval_steps_to_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_evaluate_model\u001b[0;34m(self, input_fn, hooks, checkpoint_path, name)\u001b[0m\n\u001b[1;32m    959\u001b[0m           \u001b[0mfinal_ops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m           \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_hooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m           config=self._session_config)\n\u001b[0m\u001b[1;32m    962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       _write_dict_to_summary(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/evaluation.py\u001b[0m in \u001b[0;36m_evaluate_once\u001b[0;34m(checkpoint_path, master, scaffold, eval_ops, feed_dict, final_ops, final_ops_feed_dict, hooks, config)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0meval_ops\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m   logging.info('Finished evaluation at ' + time.strftime('%Y-%m-%d-%H:%M:%S',\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    544\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m                           run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1020\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m                               run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1023\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m         logging.info('An error was raised. This may be due to a preemption in '\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moriginal_exc_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1113\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moriginal_exc_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1096\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m       \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1176\u001b[0m               \u001b[0mresults\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m               \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m               run_metadata=run_metadata))\n\u001b[0m\u001b[1;32m   1179\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_stop\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrun_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_requested\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/starttf/utils/plot_losses.py\u001b[0m in \u001b[0;36mafter_run\u001b[0;34m(self, run_context, run_values)\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"step\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreport_storage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"step\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreport_storage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreport_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                 \u001b[0mcreate_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minline_plotting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/starttf/utils/plot_losses.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"step\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreport_storage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"step\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreport_storage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreport_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                 \u001b[0mcreate_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minline_plotting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'step'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "hyper_params = Dict2Obj({\n",
    "    \"problem\": {\n",
    "        \"data_path\": \"data/mnist\",\n",
    "        \"number_of_categories\": 10\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"learning_rate\": {\n",
    "            \"type\": \"const\",\n",
    "            \"start_value\": 0.001\n",
    "        },\n",
    "        \"optimizer\": {\n",
    "            \"type\": \"adam\"\n",
    "        },\n",
    "        \"batch_size\": 64,\n",
    "        \"validation_batch_size\": 64,\n",
    "        \"steps\": 20000,\n",
    "        \"summary_steps\": 50,\n",
    "        \"save_checkpoint_steps\": 50,\n",
    "        \"keep_checkpoint_max\": 200,\n",
    "        \"checkpoint_path\": \"checkpoints\",\n",
    "        \"tf_records_path\": \"records\"\n",
    "    },\n",
    "    \"arch\": {\n",
    "        \"network_name\": \"MnistNetwork\",\n",
    "        \"dropout_rate\": 0.5\n",
    "    }\n",
    "})\n",
    "\n",
    "from starttf.estimators.scientific_estimator import easy_train_and_evaluate\n",
    "\n",
    "easy_train_and_evaluate(hyper_params, mnist_model, create_loss, inline_plotting=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5vZRDeY5rb8",
    "colab_type": "text"
   },
   "source": [
    "If you run this code on your native machine, you can visit the checkpoints path and find images there which contain plots of your metrics. In this case the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjVt8y9mr87M",
    "colab_type": "text"
   },
   "source": [
    "## Defining a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-D3lhx_tfah",
    "colab_type": "text"
   },
   "source": [
    "Next we define a model using those tensors.\n",
    "Creating a model works by passing in an input_tensor, mode, and hyper params.\n",
    "\n",
    "The mode tells the network if it has to run in evaluation, prediction or training mode.\n",
    "When in eval mode, we want to resue the weights from the training network.\n",
    "Otherwise the network would train and evaluate using different weights.\n",
    "\n",
    "The Network we want to write is a little bit ispired by vgg just smaller. We have 2 conv layers, a pooling layer, 2 conv layers a pooling layer, dropout in training and finally some fully connected layer before a softmax layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "accAmakd0_sf",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "def create_model(input_tensor, mode, hyper_params):\n",
    "    model = {}\n",
    "    \n",
    "    with tf.variable_scope('SimpleMnistNetwork') as scope:\n",
    "        # Prepare the inputs\n",
    "        x = tf.reshape(tensor=input_tensor[\"image\"], shape=(-1, 28, 28, 1), name=\"input\")\n",
    "\n",
    "        # First Conv Block\n",
    "        conv1 = tf.layers.conv2d(inputs=x, filters=16, kernel_size=(3, 3), strides=(1, 1), name=\"conv1\",\n",
    "                                 activation=tf.nn.relu)\n",
    "        conv2 = tf.layers.conv2d(inputs=conv1, filters=32, kernel_size=(3, 3), strides=(1, 1), name=\"conv2\",\n",
    "                                 activation=tf.nn.relu)\n",
    "        pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=(2, 2), strides=(2, 2), name=\"pool2\")\n",
    "\n",
    "        # Second Conv Block\n",
    "        conv3 = tf.layers.conv2d(inputs=pool2, filters=32, kernel_size=(3, 3), strides=(1, 1), name=\"conv3\",\n",
    "                                 activation=tf.nn.relu)\n",
    "        conv4 = tf.layers.conv2d(inputs=conv3, filters=32, kernel_size=(3, 3), strides=(1, 1), name=\"conv4\",\n",
    "                                 activation=tf.nn.relu)\n",
    "        pool4 = tf.layers.max_pooling2d(inputs=conv4, pool_size=(2, 2), strides=(2, 2), name=\"pool4\")\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            pool4 = tf.layers.dropout(inputs=pool4, rate=hyper_params.arch.dropout_rate, name=\"drop4\")\n",
    "\n",
    "        # Fully Connected Block\n",
    "        probs = tf.layers.flatten(inputs=pool4)\n",
    "        logits = tf.layers.dense(inputs=probs, units=10, activation=None, name=\"logits\")\n",
    "        probs = tf.nn.softmax(logits=logits, name=\"probs\")\n",
    "\n",
    "        # Collect outputs for api of network.\n",
    "        model[\"pool2\"] = pool2\n",
    "        model[\"pool4\"] = pool4\n",
    "        model[\"logits\"] = logits\n",
    "        model[\"probs\"] = probs\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmBR5Vxs1AH9",
    "colab_type": "text"
   },
   "source": [
    "Now let's train the model again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "Hvy79ETqtpTm",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "easy_train_and_evaluate(hyper_params, create_model, create_loss, inline_plotting=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_x5yya7f2HXb",
    "colab_type": "text"
   },
   "source": [
    "Congratulations!\n",
    "\n",
    "When you now want to work on a different dataset you have to do basically the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEvh33oA2Mxh",
    "colab_type": "text"
   },
   "source": [
    "## How to do that on something else than mnist?\n",
    "\n",
    "1. Write a data generator, like the mnist generator we used. (See [mnist here](https://github.com/penguinmenac3/opendatalake/blob/master/opendatalake/classification/mnist.py) or a named folder based loader [here](https://github.com/penguinmenac3/opendatalake/blob/master/opendatalake/classification/named_folders.py))\n",
    "```python\n",
    "def threadable_gen(params, stride=1, offset=0, infinite=False):\n",
    "        # This function cannot be a lambda or pooling does not work.\n",
    "        # All interactions with the outside world must be via the params object. (agian for pooling to work)\n",
    "        # yield dicts for features and labels\n",
    "```\n",
    "2. Prepare your data as we have done here using the write_data method. You can optionally pass in a data augmentation, label_preprocessing and feature_preprocessing method if you want to.\n",
    "```python\n",
    "def write_data(hyper_params,\n",
    "               prefix,\n",
    "               threadable_generator,\n",
    "               params,\n",
    "               num_threads,\n",
    "               preprocess_feature=None,\n",
    "               preprocess_label=None,\n",
    "               augment_data=None):\n",
    "```\n",
    "3. Write a model or use a predefined one. There are implementations of common models in starttf.models. You can find vgg16 there for example. If you have an awesome model, consider a pull request at [starttf project on github](https://github.com/penguinmenac3/starttf/)\n",
    "```python\n",
    "def create_model(input_tensor, mode, hyper_params):\n",
    "       # Return a model output tensors dict.\n",
    "```\n",
    "4. Write a loss like we did here to glue together your model and the labels.\n",
    "```python\n",
    "def create_loss(model, labels, mode, hyper_params):\n",
    "        # Return a losses dict (the entry for key \"loss\" is used for minimization) and metrics dict (like shown in this notebook)\n",
    "        # If you need some advanced losses, consider using starttf.losses. (alpha balancing, focus loss, mask loss, ...)\n",
    "```\n",
    "5. Use the easy_train_and_evaluate method or write your own training logic. Here we used a train_and_evaluate method, if you need some specific training that it cannot do you can use the source code [here](https://github.com/penguinmenac3/starttf/blob/master/starttf/estimators/scientific_estimator.py) and modify it to your requirements. If it is of general interest and has good code quality, consider pull requesting. ;)\n",
    "```python\n",
    "def easy_train_and_evaluate(hyper_params, create_model, create_loss, init_model=None):\n",
    "        # Init Model is a callback that you can use to initialize your model with pretrained weights just before training starts.\n",
    "```\n",
    "\n",
    "Stick to those patterns and writing a model for example 3d-detection of vehicles in realtime is done with just as many lines of code as writing an mnist network.\n",
    "Trust me on that one. I tried it myself. ;)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Starttf.ipynb",
   "version": "0.3.2",
   "views": {},
   "default_view": {},
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
